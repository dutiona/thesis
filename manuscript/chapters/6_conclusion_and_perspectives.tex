\chapter{Conclusion and continuation}
\label{chap:conclusion}

\lettrine[lines=2]{T}{he} contributions presented in this thesis by the author followed a very clear narrative arc. The
emphasis was put first on presenting the notion of generic programming (genericity), its story and how anyone can relate
in his day-to-day life, in particular when applied to image processing. Genericity is a 4-decades years old notion that
has evolved and been adopted in very modern areas of our society. Indeed, image processing is widely used to build
modern applications used all around the world. However, it was demonstrated how difficult it can be to implement
solutions relying on genericity. Indeed, there is a rule of three tying genericity, performance and ease of use. The
rule states that one can only have two of those items by sacrificing the third one. If one wants to be generic and
efficient, then the naive solution will be very complex to use with lots of parameters. If one wants a solution to be
generic and easy to use, then it will be not very efficient by default. Finally, if one wants a solution to be easy to
use and efficient then it will not be very generic. In this thesis, we endeavor to demonstrate how to break through this
rule by following three steps.

The first step, illustrated in~\cref{chap:image.algorithms.taxonomy}, was to create an inventory of image types families
as well as image processing algorithms families. The aim was to produce a comprehensive taxonomy of types (pixel, image,
structuring elements, \ldots) and algorithms related to image processing in order to be able to write concepts (in the
sense of C++ concepts). This first step delimits the perimeter of what the author means by \emph{genericity}. From this
starting point, it becomes easier to write image processing algorithms, just by relying on those concepts. Furthermore,
different concepts exist to enable algorithm implementers to leverage properties (structuring elements' decomposability,
image's buffer contiguity, \ldots) in order to achieve maximum performance.

At this point, we need to raise the reasoning level from being at the layer of pixels (low) to being at the layer of
images (higher), in order to allow fast prototyping for simple operations while guarantying very small memory footprint
and near-zero performance impact. For this reason, we expand the concept of \emph{views} from the C++ standard (2020) to
images, and we design the notion of \emph{image view}. We also make the design choice to have cheap-to-copy image (with
a shared data buffer and reference semantic) by default in order to merge concrete image and views from the user point
of view. The lazy-evaluation, that systematically happens when using views allows performance gain, e.g. when clipping
larges images. In the case where the whole image is processed, we were able to still retain very satisfactory
performances that remain stable. Also, we show through concrete use-case, such as pixel-wise algorithm and border
management how the usage of views simplify greatly how to write more complex image processing algorithms, that are
efficient by default. We finally show the limitations of this approach, with a particular focus on the speed of
traversing an image which is a mandatory use-case we must get right.

Finally, this thesis focused on how it is possible to distribute this software to the image processing community, which
is mainly working with Python. The last work concentrates its efforts on finding the best way to design a static
(compile-time, templated C++) --- dynamic (runtime, Python notebook) bridge to bring those notions (concepts and views)
to the practitioner, efficiently. This last work explores this dilemma and offers to address it with a hybrid solution
whose design is explained in-depth. This hybrid solution relies on a type-erased type which offers compatibility with a
\emph{NumPy.ndarray}. It is then able to cast itself inside an \(n \times n\) dispatcher (dimension and underlying type)
into an optimized concrete templated C++ type. This solution also explains how to write very simply the glue code
enabling already-existing algorithms (in C++) to be exposed in Python thanks to a dispatch mechanic heavily inspired
from the C++ standard (\texttt{std::visit}, \texttt{std::variant}). The aim of this solution was to regroup at a single
place in the code all the supported types into the dispatchers for maintenance purpose, as well as demanding minimal
work from algorithm implementer to expose their algorithms, all this while keeping the native performance. Indeed, no
superfluous copy is performed thanks to \emph{pybind11}'s \emph{type-caster} facility, and one cast is done from the
type-erased type to the native one. All the instructions inside the algorithm are performed on native, optimized type.
Finally, this solution offers a way to inject custom Python types into the library for prototyping purpose, thanks to a
new abstraction layer, but at the cost of heavy performance loss. The downside of this bridge is obviously the code
bloat in the resulting image processing library binary whose size explodes exponentially with the number of supported
types multiplied by the number of algorithms multiplied by the number of additional supported data (structuring
elements, label map, etc.)

We conclude this thesis by offering a new avenue of research around the Just-In-Time (JIT) compilation area to further
improve the bridge between the static and the dynamic world. The author thinks that this avenue is worth exploring,
especially with the already promising existing tools (Xeus-cling~\parencite{quantstack.2021.xeus-cling},
Cppyy~\parencite{wimtlplavrijsen.2016.cppyy}, Cython~\parencite{behnel.2010.cython},
AutoWIG~\parencite{fernique.2018.autowig}, Pythran~\parencite{guelton.2015.pythran}) in order to solve the code bloat
issue. Indeed, we would only compile what the user needs, but the entry price may be to statically link a C++
interpreter (LLVM/Cling?) into the image processing library binary which in itself would greatly bloat it. It may be
possible, however, to rely on the user's system-wide infrastructure so that the packaged image processing library does
not distribute a whole C++ interpreter/compiler alongside the image processing library binary. This is still a new area
of research and the author would very much want to delve into it to study further what is possible to achieve as of
today with those tools for the image processing community.

In addition, it would be interesting to expand the Concept framework presented in~\cref{chap:image.algorithms.taxonomy}
to encompass other areas related to computer vision, such as 3D computer graphics (CGI), visualization of 3D scenes in
real time or augmented reality. Those areas are booming and are in dire need of performance. We feel that such a
framework would be useful to improve usability of existing libraries while improving performances.

Also, it would be interesting to question if there are other programming languages, other than C++, with the required
generic capabilities for image processing and performances. For example, Rust's generic facilities, which are
trait-based  with a \texttt{where} clause, seems to be promising. Indeed, C++ has a heavy governance with inertia (it is
an ISO standard) and the language still misses some major key features such as reflection, introspection and contracts.
It took 20 years to get Concepts into the standard. The state of tooling and package management seems to be, to this
day, very clunky. There are lots of tools to do the same things in different ways, with different level of compatibility
and platform support. By contrast, Rust has a full integrated environment, and it is very pleasant to start a project
with it.

To finish with programming-related research avenue, it would be interesting to do a particular focus on heterogeneous
computing applied to algorithms canvas. Indeed, we have presented how, in theory, we can factorize optimization
opportunity with canvas. We now need to leverage this opportunity to make it possible to offload work on, for instance,
GPU or cloud infrastructure.

Another research avenue would be to find a way to validate our assertion about the library's code being easy to use in
an empirical way. Indeed, we rely on notion such as code clarity or code expressiveness, but those notions are
subjective. As we are expert in our area, our point of view about what is friendly and/or expressive is biased. It would
be interesting to make an experiment with students learning how to program, where one student pool is given a library
with inexpressive / unfriendly code (in our point of view), and another student pool is given a library with expressive
/ friendly code (in our point of view) to measure how fast they are able to program complex applications with it. If we
repeat this experiment over time, following a methodology derived from clinical trial (for ethic and preventing bias),
we will be able to empirically validate our assertion about the friendliness and expressiveness of code.

% "Understanding why software fails is important, but the real challenge is understanding why software works.
% - Alexander Stepanov
% Failure / Happens-to-work / Correct

% "The  gap between code that fails and code that is correct is  vast. Within it lies all the code that happens-to-work.
% Strive to write correct code and you will write better code."
% - Sean Parent

% One of the problem with named concepts becoming part of the language is that concepts now serve both as a requirement
% and as a guarantee. This means that there is zero accesses of freedom for the named requirements within the standard.
% This implies that if the commitee get one concept wrong, it has to come up with a new concept enterily instead of
% fixing the wrong existing one. That explain why the commitee choose to take so long: to get them all right.

% concepts, contracts and pattern matching are close related to each other.
% guelton.2015.pythran
% floyd.1993.meaning
% meyer.1992.design
% gamma.1995.design
% piper.1985.data
% bentley.1983.programming
% bentley.2016.programming
% haralick.1977.imageaccessprotocol
% bourbaki.1970.theorie
% hoare.1969.axiomatic
